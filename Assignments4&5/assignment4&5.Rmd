---
title: "R Notebook"
output: html_notebook
---
***

#Experimental Design and Data Analysis: Assignment 4
## Introduction

In the present document, the results for the third and fourth assignment of the EDDA course are presented.

## Assignment 4

First of all, we load the libraries needed for the assignments.

```{r}
library(mvtnorm)
library(multcomp)
library(lme4)
```

### Exercise 1

```{r}
bread = read.table('bread.txt')
environment=as.factor(bread$environment)
humidity=as.factor(bread$humidity)
hours=as.numeric(bread$hours)
```

#### 1

Here after is a randomization process for this database. The first element is temperature, with cold=1, intermediate=2 and warm=3; the second, humidity, where dry=1 and wet=2; and finally, the third is the time to decay in hours.

```{r}
I=3; J=2; N=3;
rand=sample(50:500, N*I*J)
rbind(rep(1:I,each=N*J), rep(1:J,N*I), rand)
```
#### 2

```{r}
boxplot(hours~environment,data=bread)
```
```{r}
boxplot(hours~humidity,data=bread)
```

```{r}
interaction.plot(environment,humidity,hours)
```
```{r}
interaction.plot(humidity,environment,hours)
```
#### 3

We will now perform an analysis of variance to test for effect of the factors temperature, humidity, and their interaction.

```{r}
breadvar=lm(hours~environment*humidity,data=bread)
anova(breadvar)
```
The analysis of variance above gives us the coefficients for the linear model. As it can be seen in the F value column, the coefficient related to the environment temperature has a value of 233.685, while the one related to humidity had a value of 62.296, which is almost four times lower. Their interaction had a value of 64.796, which is a value very close to the coefficient attached to humidity alone.

#### 4

```{r}
summary(breadvar)
```
Here, we observe in more details the coefficients for each one of the classes in each variable. In general, when the humidity is wet, it affects positively the hours for decay with coefficient 72.00; however, when combined to an intermediate or warm environment temperature, these coefficients drop to -180.00and -268.00, respectively. However, the environment temperature when intermediate or warm, affect both ways negatively the hours for decay with coefficients -124.00 and -100.00, respectively. Thus, it is not correct to afirm that either one or the other variable has the greatest numerical influence on decay, as it has been observed that the importante of humidity changes if combined with certain conditions of the environment temperature.

#### 5

The following procedure will help us to look for outliers in the dataset.

```{r}
qqnorm(residuals(breadvar))
qqline(residuals(breadvar))

plot(fitted(breadaov2),residuals(breadaov2),xlab="Fitted",ylab="Residuals")
abline(h=0)
```
From the last figure, it is clear that most of points have residuals in the linear model, however it two extreme points could be considered as outliers, one with value around 40 and the one with value around -50.



### Exercise 2

```{r}
search = read.table('search.txt')
time=as.numeric(search$time)
skill=as.numeric(search$skill)
interface=as.numeric(search$interface)
```

#### 1

Here after is a randomization process for this database. The first element is skill; the second, interface; and finally, the third is the time to find the product.

```{r}
I=5; J=3; N=1;
rand=sample(15:30, I*J)
rbind(rep(1:I,each=1), rep(1:J,each=5), rand)
```

#### 2

Here after, we will create a contigency table to better see how the two  variables "skill" and "interface" are correlated. In general, when the skill values are lower - thus, higher skills - the time to find the product is lower. Furthermore, the effect of the interfaces is also important as the time to find the product seems to increase, in general, in the direction of Interface 1-> Interface 2-> Interface 3.

```{r}
xtabs(time~interface+skill)
```

Now, we will visualize the boxplot of the two variables in regard to time. The two boxplots do help us maintain our statements made so far, as it can be seen in the two figures. 

```{r}
par(mfrow=c(1,2))
boxplot(time~interface, xlab="Interface", ylab="time")
boxplot(time~skill, xlab="Skill", ylab="time")
```

```{r}
interaction.plot(interface,skill,time)
interaction.plot(skill,interface,time)
```
Finally, we create interaction plots to better understand how these variables interact. Fixing the values of the skill values, time increases when going from interface 1 to 2, except for when the skills have value 1. The same can be said for when going from interface 2 to 3, except for when the skills have value 4.
Similarly, we analyse the interaction plot now fixing the interface values. The first interface seems to be faster used by those with skill 2; the second, by those with skill 1, and the third likewise. However, interface 3 seems to be also user-friendly for people with skill of 4, as there is an important downward slope from skill 3 to 4.

#### 3

```{r}
searchvar=lm(time~interface+skill,data=search)
anova(searchvar)
```

#### 4

```{r}
summary(aovsearch)
```

#### 5

```{r}
qqnorm(residuals(aovsearch))
qqline(residuals(aovsearch))
plot(fitted(aovsearch),residuals(aovsearch),xlab="Fitted",ylab="Residuals")
abline(h=0)
```

#### 6

```{r}
friedman.test(time,interface,skill)
```

#### 7

```{r}
search_noskill=search[,-c(2)]
search_noskill$interface=as.factor(search_noskill$interface)
aovsearch_noskill=lm(time~interface,data=search_noskill)
anova(aovsearch_noskill)
```

### Exercise 3

```{r}
cream = read.table('cream.txt')
acidity=as.factor(cream$acidity)
batch=as.factor(cream$batch)
position=as.factor(cream$position)
starter=as.factor(cream$starter)
```

#### 1

```{r}
aovcream=lm(acidity~starter+batch+position,data=cream)
anova(aovcream)
```
#### 2

```{r}
summary(aovcream)
```
#### 3

```{r}
library(multcomp)
multcream=glht(aovcream,linfct=mcp(starter="Tukey"))
summary(multcream)
summary(aovcream)
summary(multcream)
```
#### 4

```{r}
confint(multcream)
```

##EXERCISE 4
```{r}
cow = read.table("cow.txt", header=TRUE, quote="\"")
attach(cow)
```

In a study on the effect of feedingstuffs on lactation a sample of nine cows
were fed with two types of food, and their milk production was measured. All
cows were fed both types of food, during two periods, with a neutral period
in-between to try and wash out carry-over effects. The order of the types of
food was randomized over the cows.
The observed data can be found in the file cow.txt, where A and B refer to
the types of feedingstuffs.

###1. Test whether the type of feedingstuffs influences milk production using an ordinary “fixed effects” model, fitted with lm.
```{r}
fit_cow = lm(milk ~ treatment)
anova(fit_cow)
summary(fit_cow)
```
because R squered is very low and  the p value is 0.9 for treatment.

###2. Estimate the difference in milk production.
```{r}
by(cow$milk, cow$id, diff,  simplify = TRUE)
```

###3. Repeat 1. and 2. by performing a mixed effects analysis, modelling the cow effect as a random effect (use the function lmer). Compare your results to the results found by using a fixed effects model.
```{r}
library(lme4)
cowlmer=lmer(milk~treatment+order+per+(1|id),data=cow,REML=FALSE)
summary(cowlmer)
```

###4. Study the commands: attach(cow)
###t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
###Does this produce a valid test for a difference in milk production? Is its
###conclusion compatible with the one obtained in 1.? Why?
```{r}
attach(cow)
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```
comparing the result in part 1 with p value of 0.967 and result in part 4 with p value of 0.82 both null hypotesis are rejected and the models do not show that the true difference in means of treatment equals to 0



***
***
***
#Experimental Design and Data Analysis: Assignment 5
This assignment consists of 3 exercises. Throughout this assignment tests should
be performed using a level of 0.05.

##EXERCISE 1
```{r}
nauseatable = read.table("nauseatable.txt", header=TRUE, quote="\"")
```

The file nauseatable contains data about post-operative nausea after medication
against nausea. Two different medicines were administered to patients that
complained about post-operative nausea. One of the medicines, Pentobarbital,
was administered in two different doses.
###1. Set up a data.frame in R existing of two columns and 304 rows. One column should contain an indicator whether or not the patient in that row suffered from nausea, and the other column should indicate the medicin. (Use nausea.frame=data.frame(nausea,medicin) where nausea is a vector 0’s and 1’s and medicin is the vector containing the medicin labels for each patient. Make sure these columns match correctly.)
```{r}

```

###2. Study the outcome of xtabs(∼medicin+naus).
```{r}

```

###3. Perform a permutation test in order to test whether the different medicins work equally well against nausea. Permute the medicin labels for this purpose. Use as test statistic the chisquare test statistic for contingency tables, which can be extracted from the output of chisq.test: chisq.test(xtabs(∼medicin+nausea))[[1]].
```{r}
chisq.test: chisq.test(xtabs(∼medicin+nausea))[[1]].

```

###4. Compare the p-value found by the permutation test with the p-value found from the chisquare test for contingency tables. Explain the difference/equality of the two p-values.
```{r}

```


#EXERCISE 2
```{r}
airpollution = read.table("airpollution.txt", header=TRUE, quote="\"")
```

This exercise concerns the data in the file airpollution. Investigate which
explanatory variables need to be included into a linear regression model with
oxidant as the response variable. Do this as follows.


###1. Make scatter plots of the candidate explanatory variables against each other and against the response variable (see the R-function pairs()). Interpret the plots. Do you judge a linear model to be useful here?
```{r}

```

###2. Determine for each of the explanatory variables the simple linear regression model. Choose the best among these models, and stepwise extend this model by adding one explanatory variable per step on the basis of the determination coefficient. Use a test to investigate whether the extensions are useful. Determine in this way an appropriate linear regression model for these data.
```{r}

```

###3. Estimate the parameters in the full linear regression model with all explanatory variables in it. Now stepwise decrease this full model with the aid of tests of the form H0 : βi = 0. Determine in this way an appropriate linear regression model for the data.
```{r}

```

###4. Present the estimates of the parameters of the final model of your choice.
```{r}

```

###5. Investigate the normality of the residuals of the chosen model. Do you think, in view of all results, that the chosen linear model is appropriate?
```{r}

```


##EXERCISE 3
```{r}
expensescrime = read.table("expensescrime.txt", header=TRUE, quote="\"")
attach(expensescrime)
```

The data in expensescrime were obtained to determine factors related to state
expenditures on criminal activities (courts, police, etc.) The variables are:
state (indicating the state in the USA), expend (state expenditures on criminal
activities in $1000), bad (number of persons under criminal supervision),
crime (crime rate per 100000), lawyers (number of lawyers in the state), employ
(number of persons employed in the state) and pop (population of the state in
1000). Perform a regression analysis using expend as response variable and bad,
crime, lawyers, employ and pop as independent variables. Present your final
model. Your analysis should at least include:
###a. investigation of potential or influence points
```{r}
pairs(expensescrime[2:7])
```
according to the pairs graph expend and almost every other variable has some outliers so we use cooks distance to see this influence poits more clearly
```{r}
fit0 = lm(expend ~ bad + crime + lawyers +employ+pop)
round(cooks.distance(fit0),2)
plot(1:51,cooks.distance(fit0))
```
we see that 5, 8, 35 and 44 are close or higher than 1 and therfore influencal 

using car library
```{r}
library(car)
avPlots(fit0)
cutoff <- 4/((nrow(expensescrime)-length(fit0$coefficients)-2)) 
plot(fit0, which=4, cook.levels=cutoff)
influencePlot(fit0,	id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```
big circles show the influencal points that are close to 1 from 0.5 to 0.7 this confirms the previous findings


###b. investigation of problems due to collinearity
we compare xi and xj and if they have large variance and large confidence interval then they are colinear
for multiple variables we can use laso but not in this course
scatter plot X1 against all other x 
calculate linear corelation for all combinations check if they are far from 0
using cor fucntion we see pairwise colinearity and the ones near 1 are co linear

```{r}
round(cor(expensescrime[,3:7]),2)

```
results show strong corelation between pop and bad as well as pop and employ
to clarfy we will also use scatter plots to check it visually
```{r}
scatter.smooth(pop,bad)
scatter.smooth(pop,employ)
scatter.smooth(bad,employ)
```
scatter plot confirms our observation about co linear variables

###c. investigation of residuals.
```{r}
fit1 = lm(expend ~ bad + crime + lawyers +employ+pop)
summary(fit1)
```
we notice crime is not significant so we step down 
```{r}
fit2 = lm(expend ~ bad  + lawyers +employ+pop)
summary(fit2)
```
we notice  =R squared reduces slightly but no signifucant reduction
since we know pop and bad are co linear we remove one 
```{r}
fit3 = lm(expend ~ bad + crime + lawyers +employ)
summary(fit3)
```
bad and crime lost their significance so we step down by removing them
```{r}
fit4 = lm(expend ~ lawyers +employ)
summary(fit4)
```
now the R squered is still  high and close to previous models while we only have lawyers and employ as our most significant values

we are refitting the data using step up this time starting with the first variable bad
```{r}
fit1 = lm(expend ~ bad)
summary(fit1)
```
next we add crime 
```{r}
fit2 = lm(expend ~ bad +crime)
summary(fit2)
```
the R squered increases very slightly but crime has no significance so we dont concider it 
```{r}
fit3 = lm(expend ~ bad + lawyers )
summary(fit3)
```
with R squered at 0.94 we have a huge increase but the model shows that bad lost its significance so we remove it and we keep lawyer and add the next one 
```{r}
fit3 = lm(expend ~ lawyers + employ )
summary(fit3)
```
we see another increase in R squered and both lawyer and emply are significant so we add our final attribute
```{r}
fit4 = lm(expend ~ lawyers + employ +pop )
summary(fit4)
```
There is a slight increase in R squered but pop has no siginificance

You may use all techniques mentioned on the lecture slides. State clearly all
the choices you make during the regression analysis, including arguments for all
your choices. (Note that there are several strategies possible!)

 the best model for step up is fit3 = lm(expend ~ lawyers + employ ) with Multiple R-squared:  0.9632
this compares to  fit4 = lm(expend ~ lawyers +employ) for step down with Multiple R-squared:  0.9632 
both methods resulted in the same model

finaly we plot the residuals to see if they are curved 
```{r}
plot(residuals(fit3),lawyers)
plot(residuals(fit3),employ)
```

finalyl checking the model with y and the fitted y 
```{r}
plot(residuals(fit3),expend)
plot(residuals(fit3),fitted(fit3))
```
both graphs have the same spread
