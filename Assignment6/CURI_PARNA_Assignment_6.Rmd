---
title: "R Notebook assignment 6"
output: html_notebook
---

Experimental Design and Data Analysis: Assignment 6

Fabio Curi Paix?o & Arash Parna 

## EXERCISE 1

We first load the file needed for this exercise.

```{r}
fruitflies = read.table('fruitflies.txt',header = TRUE)
thorax=as.numeric(fruitflies$thorax)
longevity=as.numeric(fruitflies$longevity)
activity=as.factor(fruitflies$activity)
```

### 1 

The log of the longevity is given as follows:

```{r}
fruitflies$loglongevity <- log(as.numeric(fruitflies$longevity))
loglongevity=as.numeric(fruitflies$loglongevity)
```

### 2 

Informative plots of the data are given here after.

```{r}
plot(thorax~longevity,pch=unclass(activity))
plot(longevity~thorax,pch=as.character(activity))
```
, where in the second plot the circles represent "high", triangles are "isolated" and the crosses are "low", which are outputs for "activity".

### 3

We build the following intercept-free model:

```{r}
fit1 = lm(longevity~activity-1,data=fruitflies)
summary(fit1)
anova(fit1)
fit1
```
By the results just obtained from the intercept and the coefficients for isolated and low activities, it is safe to say that sexual activity influences longevity.

### 4

The final model is given as follows:

longevity = 38.720*activityhigh + 63.560*activityisolated + 56.760*activitylow + error

With these positive coefficients, it is safe to state that sexual activity increase longevity for the three cases. The estimated longevities in days for the three conditions are the following:

longevity(high) = 38.720*1 + 63.560*0 + 56.760*0 + error = 38.720 + error

longevity(isolated) = 38.720*0 + 63.560*1 + 56.760*0 + error = 63.560 + error

longevity(low) = 38.720*0 + 63.560*0 + 56.760*1 + error = 56.760 + error

### 5

Now, we will build the model with both explanatory variables.

```{r}
fit2= lm(longevity~activity-1+thorax,data=fruitflies)
summary(fit2)
```

The results of this new model induce us to think differently. The coefficients for the different sexual activities became negative, while the one for thorax length is positive. From the values in the last column, which are very low, it is still safe to state that both variables are influential in the model.

### 6

From this new model, sexual activity appears to decrease longevity. The final model is the following:

longevity = -67.37*activityhigh -47.31*activityisolated -54.32*activitylow + 132.62*thorax + error

The average and smallest thorax length in the dataset are the following:

```{r}
mean(thorax)
min(thorax)
```
The longevity in days for a fly with average thorax length is the following:

longevity(high)= -67.37*1 -47.31*0 -54.32*0 + 132.62*0.82 + error = 41.38 + error

longevity(isolated)= -67.37*0 -47.31*1 -54.32*0 + 132.62*0.82 + error = 61.44 + error 

longevity(low)= -67.37*0 -47.31*0 -54.32*1 + 132.62*0.82 + error = 54.43 + error

And for a fly with the smallest thorax length:

longevity(high)= -67.37*1 -47.31*0 -54.32*0 + 132.62*0.64 + error = 17.51 + error

longevity(isolated)= -67.37*0 -47.31*1 -54.32*0 + 132.62*0.64 + error = 37.57 + error

longevity(low)= -67.37*0 -47.31*0 -54.32*1 + 132.62*0.64 + error = 30.56 + error

### 7

```{r}
plot(longevity~thorax,pch=unclass(activity))
abline(lm(longevity~thorax,data=fruitflies[fruitflies$activity=='isolated',]))
abline(lm(longevity~thorax,data=fruitflies[fruitflies$activity=='low',]))
abline(lm(longevity~thorax,data=fruitflies[fruitflies$activity=='high',]))
```
, where the abline plots are, from up to bottom, fitting the isolated, low and high sexual activities. Thus, when considering the thorax length, the plot shows three upward linear behaviours for the three cases. Thus, they have simmilar behaviours, however the three conditions have higher abline fit values from isolated towards high sexual activities, passing through low sexual activity.

### 8

```{r}

```

### 9

```{r}
qqnorm(residuals(fit2))
plot(fitted(fit2),residuals(fit2))
```

### 10

```{r}

```

##EXERCISE 2

```{r}
psi_table = read.table('psi.txt',header = TRUE)
passed = as.numeric(psi_table$passed)
gpa = as.numeric(psi_table$gpa)
psi = as.numeric(psi_table$psi)
```


### 1

```{r}
par(mfrow=c(1,2))
boxplot(gpa~passed)
boxplot(gpa~psi)
```
We can observe here that the "gpa" was considerably higher for those who passed the exam, which is expected. Now, a more interesting analysis is done, which indicates that the median in the boxplot for those who obtained the "psi" was higher. Students who obtained training with or without this method managed, nevertheless, to reach the maximum score of 4. Furthermore, within the group of students who were offered the "psi" training, it was observed the lower overall score ("gpa" around 2.1).

```{r}
par(mfrow=c(1,2))
interaction.plot(passed,psi,gpa)
interaction.plot(psi,passed,gpa)
```

These plots are very interesting because they show the influence of the psi methodology on the students' overall score in the assignment. For those having been under the "psi" training, the mean of "gpa" is always observed to be lower than the mean of those who have not been trained with psi. Furthermore, among those who passed the assignment, the mean of "gpa" is lower for those who have had the psi training. The same stands for those who have not passed the assignment.

### 2

Here we fit a logistic model for this example.

```{r}
psi_table$gpa2=gpa^2
psiglm=glm(passed~gpa+gpa2+psi,data=psi_table,family=binomial)
summary(psiglm)

psiglm2=glm(passed~gpa+psi,data=psi_table,family=binomial)
summary(psiglm2)
```
The 2 explanatory variables are inserted as numerical variables. The positive signs of the parameter estimates mean that higher values of these variables give higher probability that the psi method was applied. A very interesting observation is that, in general, for higher values of gpa", the lower is the probability that the "psi" method was applied.

### 3

Now we can conclude from the two previous subsections that the "psi" method does work with regards to being efficient in enabling the students to pass the exam, however their grades are not as good as those who have not been submitted the "psi" methodology, in general.

### 4

The probability that a student with a "gpa" equal to 3 who receives "psi" passes the assignment is given as follows:

```{r}
# psiglm=glm(passed~gpa+psi,data=psi_table,family=binomial)
# gpa3passed=data.frame(psi=factor(1),gpa=3)
# predict.glm(psiglm,gpa3passed,type="response")


psi_table$gpa <- as.numeric(psi_table$gpa)
psiglm=glm(passed~gpa+psi,data=psi_table,family=binomial)

gpa3passed=data.frame(psi=factor(1),gpa=3)
pass = predict.glm(psiglm,gpa3passed,type="response")
pass

```

And for those who have not received "psi":

```{r}
gpa3fail=data.frame(psi=factor(0),gpa=3)
fail = predict.glm(psiglm,gpa3fail,type="response")
fail
```

Thus, this confirms that "psi" is a good methodology to make students pass.
students with a gpa of 3 who recived psi are at least 15 times more likley to pass
### 5

```{r}
psi_table$gpa <- as.numeric(psi_table$gpa)
psiglm=glm(passed~psi,data=psi_table,family=binomial)

gpa3passed=data.frame(psi=factor(1))
pass = predict.glm(psiglm,gpa3passed,type="response")
pass
gpa3fail=data.frame(psi=factor(0))
fail = predict.glm(psiglm,gpa3fail,type="response")
fail

```


###6. Do this. What are the numbers 15 and 6 in this table? What is the conclusion?
18 no psi 
3 improve
15 not improved

14 with psi
8 improve
not improved

15 student who did not recive psi and did not show improvment
6 student who did recive psi and did not show improvment 

```{r}
x=matrix(c(3,15,8,6),2,2) 
x
fisher.test(x)
```
fisher test null hypotesis that students in group p1 have the same probibility as students in p2 is 0.02 < 0.05 and rejected
conclusion is that there is a different between the student who did recive psi and those who did not 

###7. Given the way the experiment was conducted, is this second approach wrong? Why or why not?

althought both experiments produce the same conclusion but the fisher test experiment does not concider the gpa of the participants and therefore its the wrong approach but not nessecerily wrong. the second approach only tests the effectivness of psi without concidering other attributes such as gpa.

###8. Name both an advantage and a disadvantage of the two approaches, rela- tive to each other.

the advantage of using the first method is that if the data set has more than 2 rows and columns we can use the first method while the second methos needs a 2 by 2 grid. 
methos one tests the dependency between response variable and other variables 
while method 2 only shows if there is a difference between 2 kinds of classification and often used in small sample sizes. 
method 2 is not used for prediction while method 1 can be used for prediction.



## EXERCISE 3

```{r}
africa = read.table('africa.txt',header = TRUE)
```


### 1
```{r}
par(mfrow=c(4,4))

hist(rpois(10,0.5))
hist(rpois(10,5))
hist(rpois(10,100))
hist(rpois(10,1000))

hist(rpois(100,0.5))
hist(rpois(100,5))
hist(rpois(100,100))
hist(rpois(100,1000))

hist(rpois(1000,0.5))
hist(rpois(1000,5))
hist(rpois(1000,100))
hist(rpois(1000,1000))

hist(rpois(10000,0.5))
hist(rpois(10000,5))
hist(rpois(10000,100))
hist(rpois(10000,1000))

```
What can be observed here is that, keeping n fixed and varying lambda, we obtain different histograms. The larger the value of lambda, the larger the values of Y on average and the larger also the spread in the values of Y. For high lambdas, the Poisson(lambda)-distributionis approximately equal to a normal distribution.

Now, keeping lambda fixed and varying n, we obtain again new histograms.
We can see that the dimension of the x-axis stays approximately the same, since the lambda is the same. However, a higher number of the population makes the distribution of the values around this same population more equal. This does not mean that the outputs of the rpois function is the same for all elements, only that it is better distributed / more normaly distributed 


### 2 Two distributions are in the same location-scale family if there is a scale- and-shift transformation that maps one to the other. Investigate whether different Poisson distributions are in the same location-scale family, like all normal distribution. Clearly explain your approach to this question, and your answer.
```{r}
par(mfrow=c(1,4))

hist(rpois(1000,1000)+ 100000)
hist(rpois(1000,100000))
```

### 3

```{r}
attach(africa)
miltcoup=as.numeric(africa$miltcoup)
oligarchy=as.numeric(africa$oligarchy)
pollib=as.numeric(africa$pollib)
parties=as.numeric(africa$parties)
pctvote=as.numeric(africa$pctvote)
popn=as.numeric(africa$popn)
size=as.numeric(africa$size)
numelec=as.numeric(africa$numelec)
numregim=as.numeric(africa$numregim)

africa_fullmodel=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,family=poisson,data=africa)
summary(africa_fullmodel)

confint(africa_fullmodel)
coef(africa_fullmodel)
```

The results of this model shows that many variables might not be appropriate for this model. This is explained by the high values on the last column in the summary of the model. For many variables, these values are a lot above 0.05. Thus, a stepwise decrease procedure would be adequate for this model.

### 4


Now, we must check whether the coefficients are individually equal to zero (hypothesis H0) in the stepwise decrease method. As we can see in the last subsection, the last column in the linear model report has the highest value for the variable 'numelec'. Thus, this last is deleted as it is higher than 0.05. Thus, we perform the test again without this variable.

```{r}
africa_model=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim,family=poisson,data=africa)
summary(africa_model)
```
The same now applies for the variable "numregim". The new test follows.
```{r}
africa_model=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size,family=poisson,data=africa)
summary(africa_model)
```
Now, eliminating the variable "size".
```{r}
africa_model=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn,family=poisson,data=africa)
summary(africa_model)
```
Eliminating "popn":
```{r}
africa_model=glm(miltcoup~oligarchy+pollib+parties+pctvote,family=poisson,data=africa)
summary(africa_model)
```
and "pctvote":
```{r}
africa_model=glm(miltcoup~oligarchy+pollib+parties,family=poisson,data=africa)
summary(africa_model)
```

Now, all the variables in the model are significant, as their coefficients are lower than 0.05. We end up with the variables "oligarchy", "pollib" and "parties" as explanatory variables for the output "miltcoup". Thus, many variables were deleted from the model.

### 5

```{r}
par(mfrow=c(1,2))
plot(fitted(africa_model),residuals(africa_model))
plot(log(fitted(africa_model)),residuals(africa_model))
par(mfrow=c(1,2))
plot(log(fitted(africa_model)),residuals(africa_model,type="response"))

par(mfrow=c(1,2))
plot(fitted(africa_fullmodel),residuals(africa_fullmodel))
plot(log(fitted(africa_fullmodel)),residuals(africa_fullmodel))
par(mfrow=c(1,2))
plot(log(fitted(africa_fullmodel)),residuals(africa_fullmodel,type="response"))
```

#plots dont have any specifit structure so they are ok for first 4 plots and the final 2 plots show an increase of 